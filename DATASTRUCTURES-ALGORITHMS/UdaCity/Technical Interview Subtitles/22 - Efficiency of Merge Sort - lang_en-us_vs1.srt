1
00:00:00,590 --> 00:00:03,880
So how many overall
comparisons was that?

2
00:00:03,880 --> 00:00:06,680
Well, let's try to find a pattern here.

3
00:00:06,680 --> 00:00:08,500
It looks like at each step,

4
00:00:08,500 --> 00:00:12,510
we did one less comparison than
the array we were building.

5
00:00:12,510 --> 00:00:17,300
So when we were building an array of
seven, we only did six comparisons.

6
00:00:17,300 --> 00:00:21,586
When we're building an array of three,
we did two comparisons, an array of two,

7
00:00:21,586 --> 00:00:25,299
one comparison, and an array of four,
we did three comparisons.

8
00:00:26,430 --> 00:00:27,580
I'm going to go ahead and

9
00:00:27,580 --> 00:00:31,259
call the length of each array m just
to make it a little easier for me.

10
00:00:32,409 --> 00:00:36,510
So if m is the size of
the array that we're building,

11
00:00:36,510 --> 00:00:40,440
the number of comparisons is always
going to be one less than that.

12
00:00:40,440 --> 00:00:44,470
It's going to be hard to get
an exact number of comparisons here.

13
00:00:44,470 --> 00:00:49,120
However, we have a good friend that
we can count on, approximation.

14
00:00:49,120 --> 00:00:52,379
Since we know that every iteration
just cycles through the same

15
00:00:52,380 --> 00:00:56,730
seven elements again and again,
we know that the size of our arrays,

16
00:00:56,730 --> 00:01:01,500
or m, are always going to
add up to seven at the end.

17
00:01:01,500 --> 00:01:03,989
So again,
here we have an array of seven.

18
00:01:03,990 --> 00:01:08,191
Here we have an array of four and an
array of three, which adds up to seven.

19
00:01:08,191 --> 00:01:12,370
And here, one, two, two,
and two adds up to seven.

20
00:01:13,410 --> 00:01:14,780
Since we're just doing divide and

21
00:01:14,780 --> 00:01:18,780
conquer, splitting up the array at each
step, we can say that approximately

22
00:01:18,780 --> 00:01:21,400
we're going to do seven
comparisons at each step.

23
00:01:22,380 --> 00:01:24,240
Again, seven is an upper bound.

24
00:01:24,240 --> 00:01:27,390
We're never going to do more
than seven comparisons, but

25
00:01:27,390 --> 00:01:29,260
at many steps we get close to seven.

26
00:01:30,740 --> 00:01:34,890
So to get the run time of a sorting
algorithm, you can normally multiply

27
00:01:34,890 --> 00:01:40,370
the number of overall iterations by the
number of comparisons at each iteration.

28
00:01:40,370 --> 00:01:44,940
In bubble sort,
we were doing n comparisons and n steps.

29
00:01:44,940 --> 00:01:48,750
We just proved that we're doing at
most n comparisons at every step.

30
00:01:49,940 --> 00:01:52,500
But how many steps overall are we doing?

31
00:01:53,180 --> 00:01:55,735
Okay, so to sort an array of seven,

32
00:01:55,735 --> 00:02:00,600
we had to do three steps,
one, two, and three.

33
00:02:01,600 --> 00:02:04,475
Also I should note some of
the sub-processes that happened here.

34
00:02:05,485 --> 00:02:09,520
In order to sort this array of two,
we only had to do one step.

35
00:02:11,140 --> 00:02:13,455
When we were sorting
an array of three or

36
00:02:13,455 --> 00:02:17,550
an array of four,
we needed to do two steps.

37
00:02:17,550 --> 00:02:20,295
We can use a nice table to
keep track of all of that.

38
00:02:20,295 --> 00:02:25,350
Again, so when our array size was seven,
we had to do three different steps.

39
00:02:25,350 --> 00:02:30,230
When our array was three or four
numbers large, we had to do two steps.

40
00:02:30,230 --> 00:02:33,852
When our array was two large,
we had to do one step.

41
00:02:33,853 --> 00:02:38,500
And if we just had one element, we don't
need to do any comparisons, so no steps.

42
00:02:39,342 --> 00:02:43,600
I'm going to go ahead and fill in a few
numbers here just for the sake of time.

43
00:02:43,600 --> 00:02:46,420
You can assume that I did these
calculations on my own and

44
00:02:46,420 --> 00:02:48,609
I encourage you to do them as well.

45
00:02:48,610 --> 00:02:52,100
Okay, so this might look
a little bit familiar to you.

46
00:02:52,100 --> 00:02:56,300
Remember in binary search when
we had a similar table and

47
00:02:56,300 --> 00:03:00,700
the number of iterations
incremented at every power of two.

48
00:03:00,700 --> 00:03:03,679
Well, we actually have
something pretty similar here.

49
00:03:03,679 --> 00:03:05,912
Instead of incrementing
at the powers of two,

50
00:03:05,912 --> 00:03:08,845
this time we're incrementing
one after every power of two.

51
00:03:08,845 --> 00:03:11,137
[BLANK_AUDIO]

52
00:03:11,137 --> 00:03:13,540
So, in the binary search video,

53
00:03:13,540 --> 00:03:16,850
I proved why the number of
iterations would be equal to log(n).

54
00:03:16,850 --> 00:03:18,810
So if you need a refresher,
you can go back there.

55
00:03:20,550 --> 00:03:23,230
Again, this time is a little
different because it increments

56
00:03:23,230 --> 00:03:26,220
after the power of two instead
of at the power of two.

57
00:03:26,220 --> 00:03:29,765
Yet again, we're kind of
relying on approximation here.

58
00:03:29,765 --> 00:03:33,665
We don't exactly care
where the change happens

59
00:03:33,665 --> 00:03:36,655
as long as we capture the interval
at which the change is happening.

60
00:03:38,195 --> 00:03:45,594
So in summary, we're doing roughly
n comparisons for log(n) steps.

61
00:03:45,594 --> 00:03:49,236
That makes an overall
complexity of nlog(n).

62
00:03:49,236 --> 00:03:52,570
This is definitely better than the n
squared we got in bubble sort.

63
00:03:53,610 --> 00:03:58,640
In bubble sort, we had n times n or
n squared as our efficiency.

64
00:04:00,105 --> 00:04:02,840
log(n) is generally going to
be less than n, but

65
00:04:02,840 --> 00:04:05,650
it's definitely never going
to be greater than n.

66
00:04:05,650 --> 00:04:08,450
So we can say that the efficiency
of merge sort is better than

67
00:04:08,450 --> 00:04:09,750
the efficiency of bubble sort.

68
00:04:11,800 --> 00:04:15,800
However, the space efficiency of merge
sort is actually worse than bubble sort.

69
00:04:16,260 --> 00:04:18,630
In bubble sort,
we were sorting in place, so

70
00:04:18,630 --> 00:04:20,690
we didn't need to use any extra arrays.

71
00:04:21,860 --> 00:04:24,770
Here we frequently copied
our values into new arrays.

72
00:04:26,500 --> 00:04:32,420
You can say that the auxiliary space or
the extra space we used was linear.

73
00:04:32,420 --> 00:04:36,909
This complexity assumes that we got rid
of arrays after we were done using them.

74
00:04:36,909 --> 00:04:40,605
At each step, we were copying
values into new arrays, so

75
00:04:40,605 --> 00:04:42,926
we needed new arrays at some point.

76
00:04:42,926 --> 00:04:46,195
However, we could say that we were
getting rid of the old ones whenever we

77
00:04:46,196 --> 00:04:47,340
were done.

78
00:04:47,340 --> 00:04:49,940
So we're not using a ton
of new arrays every time.

79
00:04:49,940 --> 00:04:54,210
We really only need two different arrays
at every step, the array that our

80
00:04:54,210 --> 00:04:57,400
numbers were in and the new array
that we're copying the values into.
